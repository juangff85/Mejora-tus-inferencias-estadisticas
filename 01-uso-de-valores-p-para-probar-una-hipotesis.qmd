**1 Uso de los valores *p* para contrastar una hipótesis**

**1 Uso de los valores *p* para contrastar una hipótesis**

Los científicos intentan responder a preguntas muy diversas recopilando
datos. Una de las preguntas que interesan es si las mediciones recogidas
en condiciones distintas **difieren o no**. La respuesta a esta pregunta
es una **afirmación ordinal**: el investigador declara que la media de
las mediciones es mayor, menor o igual al comparar condiciones.

Por ejemplo, alguien podría interesarse por la hipótesis de que los
estudiantes **aprenden mejor si hacen pruebas**(condición A), que
requieren recuperar información previamente aprendida, en comparación
con **no hacer pruebas** y dedicar todo el tiempo a estudiar (condición
B). Tras recoger datos y observar que la media de notas es mayor en
quienes hicieron pruebas, el investigador puede afirmar que el
rendimiento fue mejor en A que en B. Las afirmaciones ordinales solo
indican que **hay una diferencia**; **no cuantifican** el tamaño del
efecto.

Para realizar estas afirmaciones ordinales, los investigadores suelen
usar un **procedimiento metodológico** conocido como **prueba de
hipótesis**. Una parte de esa prueba consiste en **calcular un
valor *p*** y comprobar si existe una diferencia **estadísticamente
significativa**. "Significativo" aquí quiere decir **digno de
atención**. La prueba de hipótesis ayuda a separar la **señal** (lo que
merece atención) del **ruido** aleatorio en los datos. Es útil
distinguir entre **significación estadística**(solo dice si el efecto
observado es señal o ruido) y **significación práctica** (si el tamaño
del efecto es lo bastante grande como para tener consecuencias
relevantes). Este procedimiento sirve como salvaguarda frente al **sesgo
de confirmación**: nuestras ganas de ver confirmadas nuestras ideas
pueden llevarnos a interpretar los datos como apoyo a la hipótesis
incluso cuando no lo son.

**1.1 Enfoques filosóficos de los valores *p***

Antes de ver cómo se calculan, conviene revisar **cómo ayudan a hacer
afirmaciones ordinales** al contrastar hipótesis. Un valor *p* es **la
probabilidad de observar los datos muestrales, o más extremos, asumiendo
que la hipótesis nula es verdadera**. Esta definición, por sí sola, no
indica cómo interpretarlo. La **interpretación depende de la filosofía
estadística** que adoptemos. En el enfoque de **Fisher**, el
valor *p* es una **medida continua y descriptiva de
compatibilidad** entre los datos observados y el **modelo nulo**: cuanto
más pequeño es el *p*, mayor es la reticencia a aceptar la hipótesis
nula. Este planteamiento (pruebas de significación)
evalúa **incompatibilidades del modelo** y no especifica una hipótesis
alternativa. Fisher intentó formalizarlo como **inferencia fiducial**,
hoy minoritaria frente a teorías de decisión, verosimilitud o inferencia
bayesiana. [[lakens.github.io]{.underline}](https://lakens.github.io/statistical_inferences/01-pvalue.html)

En el enfoque **Neyman-Pearson**, se **especifican H₀ y H₁** y las
pruebas **guían la acción**: si *p* es menor que α, se actúa "como si"
H₁ fuera cierta (el tamaño exacto de *p* por debajo de α no cambia la
decisión). Este marco no pretende cuantificar evidencia continua,
sino **regular decisiones** sobre qué afirmaciones formular. En la
práctica, muchas áreas usan un **híbrido** de ambos enfoques.

**1.2 Creación de un modelo nulo**

Imagina dos grupos de 10 personas que puntúan la edición extendida
de *El Señor de los Anillos*. La media de mis amistades es 8,7 y la de
las amistades de mi pareja 7,7: **diferencia = 1 punto**. La pregunta es
si esa diferencia es solo **variación aleatoria** o si
podemos **afirmar** que a mi grupo le gusta más. En un contraste NHST
calculamos la **probabilidad** de observar esa diferencia (o una
mayor) **asumiendo que en la población la diferencia real es 0**: eso es
el **valor *p***. Si es suficientemente pequeño, **emitimos la
afirmación**; si no, nos **abstenemos**. Para cuantificar **qué
diferencias caben esperar solo por azar**, construimos un **modelo
nulo**. Suele ser útil expresarlo en una distribución estandarizada; p.
ej., la **t de Student** (aquí con 18 g.l.) bajo supuestos
como **normalidad**. En la práctica los supuestos nunca se cumplen
perfectamente, por eso se estudia el **impacto de sus violaciones**: si
es pequeño, los tests siguen siendo útiles. Como las probabilidades en
una distribución continua se definen sobre **intervalos**, el *p* es la
probabilidad de los **datos "iguales o más extremos"** (la cola de la
distribución). [[lakens.github.io]{.underline}](https://lakens.github.io/statistical_inferences/01-pvalue.html)

**1.3 Cálculo de un valor *p***

En una **t de dos muestras**, el estadístico se obtiene a partir
de **medias, desviaciones estándar y tamaños muestrales** de cada grupo;
el *p* es la probabilidad de observar un **\|t\| igual o mayor** al
observado bajo el modelo nulo. En el ejemplo de las puntuaciones de la
película, una prueba **t de Student bilateral** produce **t =
2,5175** con **p = 0,02151** (g.l. = 18).

**1.4 ¿Qué valores *p* puedes esperar?**

Los *p*-valores **varían de un experimento a otro**. Esto no significa
que "no haya que fiarse del *p*", sino que conviene **entender su
distribución** para evitar confusiones. En el marco frecuentista, lo que
importa es **qué ocurre a largo plazo**:

-   Cuando **H₀ es verdadera** y el **estadístico de prueba es
    continuo** (p. ej., una *t*), los *p*-valores siguen
    una **distribución uniforme** en (0, 1).

-   Si el estadístico es **discreto** (p. ej., ciertos usos del
    ji-cuadrado), la distribución de *p* **no** es
    uniforme. [[lakens.github.io]{.underline}](https://lakens.github.io/statistical_inferences/01-pvalue.html)

**1.5 La paradoja de Lindley**

Al **aumentar la potencia**, la distribución de *p* se sesga hacia la
derecha: los *p* muy pequeños pasan a ser más probables **si hay
efecto**. Sin embargo, con **potencias muy altas** puede suceder que
un *p* = 0,04 sea **más probable bajo H₀ que bajo H₁**: esto es
la **paradoja de Lindley**. Así, un resultado "p \< .05" puede ser
significativo en el sentido Neyman-Pearson (control de error ≤ α),
pero **no** constituir la evidencia más favorable para H₁ desde marcos
de **verosimilitud** o **bayesianos**. Una estrategia para reducir estas
situaciones es **bajar α en función del tamaño muestral**. 

Desde una perspectiva estricta de Neyman--Pearson bastaría con decir
si *p* \< α o *p* \> α, **pero conviene reportar el *p*exacto**:
facilita reanálisis y permite que otros comparen ese *p* con el α que
habrían preferido. Como las afirmaciones se hacen mediante
un **procedimiento con tasas máximas de error conocidas**,
un *p* **no** permite afirmar nada "con certeza". Incluso con α =
0.000001, **cualquier afirmación aislada puede ser errónea**; por eso
la **replicación** es importante. Esta incertidumbre a veces **no** se
refleja en expresiones como "probar", "demostrar" o "se sabe". Una
redacción más precisa tras un NHST **significativo** sería:

"Afirmamos que hay un efecto distinto de cero, reconociendo que, si los
científicos formulan afirmaciones con este procedimiento, se engañarán,
a largo plazo, como mucho un α % de las veces (lo que consideramos
aceptable). Supondremos, hasta que nuevos datos lo refuten, que esta
afirmación es correcta."

Tras un NHST **no significativo**:

"No podemos afirmar que exista un efecto distinto de cero, reconociendo
que, si se evita afirmar con este procedimiento, se errará, a largo
plazo, como mucho un β % de las veces."

Ojo: **no** podemos afirmar ausencia de efecto tras un resultado no
significativo; conviene realizar también una **prueba de
equivalencia** (o, si hiciste una **prueba de efecto mínimo**, reemplaza
"distinto de cero" por ese umbral).

**1.7 Prevenir malentendidos comunes sobre los valores *p***

Antes de entrar en los malentendidos, el texto introduce la **idea de
modelo alternativo** (además del nulo). Al aumentar el **tamaño
muestral**, el **error estándar** se reduce y el modelo nulo "se
estrecha"; por eso una misma diferencia observada (p. ej., 0,5 puntos)
resulta **mucho más sorprendente** con *n* grande que con *n* pequeño.
Algunos programas (p. ej., **G\*Power**) muestran en un mismo gráfico
el **modelo nulo** (curva roja), el **alternativo** (azul) y el **valor
crítico** que separa resultados significativos de no significativos. El
texto usa la figura de "**Omniscient Jones**" (quien conoce la
diferencia verdadera) para razonar sobre potencia y
evidencias. [[Lakens+1]{.underline}](https://lakens.github.io/statistical_inferences/01-pvalue.html)

**1.7.1 Malentendido 1: "Un *p* no significativo (p \> .05) significa
que H₀ es verdadera."**

No. Un *p* no significativo indica que **no** hay base suficiente (bajo
ese procedimiento y α) para **afirmar** una diferencia; **no**confirma
que la diferencia sea exactamente cero. Si quieres sostener "no hay
efecto relevante", necesitas, por ejemplo, una **prueba de
equivalencia** con márgenes previamente definidos. [Lakens]{.underline}

**1.7.2 Malentendido 2: "Un *p* significativo (p \< .05) demuestra que
H₀ es falsa."**

Tampoco. Un resultado significativo solo dice que **es
improbable** observar datos así **si H₀ fuese cierta**. El marco de
Neyman--Pearson guía **decisiones** (actuar "como si" H₁ fuera cierta
cuando *p* \< α), pero **no** cuantifica una probabilidad de verdad de
las hipótesis. [Lakens]{.underline}

**1.7.3 Malentendido 3: "Significativo = importante."**

No necesariamente. "Significativo" equivale mejor a **"sorprendente bajo
H₀"**. La **relevancia práctica** exige mirar **tamaños del
efecto**, **intervalos de confianza** y el **contexto**. Un efecto
diminuto puede ser significativo con *n* grande y, aun así, **poco
útil**. [Lakens]{.underline}

**1.7.4 Malentendido 4: "Si hallas un resultado significativo, la
probabilidad de que sea un falso positivo es 5%."**

Ese 5% es la **tasa a largo plazo** cuando **H₀ es
verdadera** (α), **antes** de ver los datos. Si supiéramos que H₀ es
cierta, **todos** los significativos serían falsos positivos (100%).
Tras observar *p* \< α, la pregunta "¿cuál es la probabilidad de que H₀
sea verdadera?" **no** la responde el *p*; requeriría información previa
(p. ej., un marco bayesiano). [Lakens]{.underline}

**1.7.5 Malentendido 5: "1 − *p* es la probabilidad de replicación."**

No. La probabilidad de obtener un resultado significativo en una réplica
depende de la **potencia** (si el efecto existe), de α y de otros
factores (variabilidad, diseño, ejecución...). **No puede derivarse de 1
− *p***. La manera de saber si un efecto "replica" es **replicarlo**.

**1.8 Ponte a prueba**

**1.8.1 Preguntas sobre qué *p*-valores puedes esperar**

**Instrucciones.** Copia este código en **R** y ejecútalo. El script
simula 100 000 estudios, hace una **t** de una muestra contra 100 y
dibuja el histograma de *p*-valores, mostrando también la potencia
aproximada. [Lakens]{.underline}

nsims \<- 100000 \# número de simulaciones

m \<- 106 \# media muestral simulada

n \<- 26 \# tamaño muestral

sd \<- 15 \# desviación típica de los datos simulados

p \<- numeric(nsims) \# vector vacío para p-valores

bars \<- 20 \# número de barras del histograma

for (i in 1:nsims) { \# para cada \"experimento\"

x \<- rnorm(n = n, mean = m, sd = sd)

z \<- t.test(x, mu = 100) \# t de una muestra

p\[i\] \<- z\$p.value \# guardamos el p-valor

}

power \<- round((sum(p \< 0.05) / nsims), 2) \# potencia

\# Gráfico

hist(p,

breaks = bars, xlab = \"p-valores\", ylab = \"número de p-valores\\n\",

axes = FALSE, main = paste(\"Distribución de p con\",

round(power \* 100, 1), \"% de potencia\"),

col = \"grey\", xlim = c(0, 1), ylim = c(0, nsims))

axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))

axis(side = 2, at = seq(0, nsims, nsims / 4),

labels = seq(0, nsims, nsims / 4), las = 2)

abline(h = nsims / bars, col = \"red\", lty = 3) \# línea roja = alfa
del 5%

**Q1.** La potencia es la probabilidad de observar un resultado
significativo cuando hay efecto real. ¿Dónde la "ves" en la figura?

-   a\) Contando *p* \> 0.5 y dividiéndolos entre el total.

-   b\) Contando los *p* de la **primera barra** (0.00--0.05) y
    dividiéndolos entre el total.

-   c\) Restando *p* \> 0.5 menos *p* \< 0.5 y dividiéndolo entre el
    total.

-   d\) Restando *p* \> 0.5 menos *p* \< 0.05 y dividiéndolo entre el
    total.

**Q2.** Cambia n \<- 26 por n \<- 51 y ejecuta. ¿Qué potencia observas
ahora (elige la más cercana)?\
55% · 60% · 80% · 95%

**Q3.** ¿Cómo cambia la distribución de *p*-valores respecto a la de 50%
de potencia?

-   Igual · Mucho más **empinada** · Mucho más **plana** ·
    Más **normal** (campana)

**Q4.** Pon m \<- 100 (sin efecto real). Ejecuta. ¿Qué notas?

-   Igual que con 50% de potencia · Más empinada · **Básicamente
    plana** (salvo ruido) · Normal (campana)

**Q5.** En el caso sin efecto real, mira la **primera
barra** (0.00--0.01 si luego aumentas barras). ¿Cómo se llama
formalmente?

-   Potencia (verdaderos positivos) · Verdaderos negativos · **Error de
    Tipo I** (falsos positivos) · Error de Tipo II (falsos negativos)

Ahora céntrate en *p* \< .05: cambia bars \<- 20 por bars \<- 100 y xlim
= c(0, 1) por xlim = c(0, 0.05). Con m \<- 100 verás
la **uniformidad** (cada barra ≈1%). Luego pon m \<- 107, n \<- 51.

**Q6.** Con m \<- 107, la potencia ronda \~90,5% para α = .05. Si bajas
α a .01, ¿qué potencia aproximada ves?\
\~90% · \~75% · \~50% · \~5%

Para examinar *p* entre 0.04 y 0.05, ajusta ylim = c(0, 10000). Pon m
\<- 108, deja n \<- 51.

**Q7.** Con **potencia muy alta** (\~96--98%), un *p* = 0.045...

-   a\) Es significativo y **apoya fuertemente** H₁.

-   b\) Es significativo y **sin duda** es Tipo I.

-   c\) Con alta potencia **deberías** usar α \< .05; por tanto, no es
    significativo.

-   d\) Es significativo, **pero** esos datos son **más probables bajo
    H₀** que bajo H₁.

**Q8.** Juega variando n y/o m. Para la barra 0.04--0.05 (línea roja =
1% bajo H₀), ¿cuánto más alta puede llegar a ser, en el mejor caso, bajo
H₁?\
Igual · \~4× · \~10× · \~30×

Conclusión de esta parte: los *p* justo por debajo de .05, **en el mejor
caso**, son como mucho un apoyo **débil** a H₁; conviene replicar o ser
cauto al interpretarlos. [Lakens]{.underline}

**1.8.2 Preguntas sobre malentendidos del *p*-valor**

Responde y contrasta con tus simulaciones o con la app enlazada en el
capítulo. [Lakens]{.underline}

**Q1.** Con *t* independiente y **n = 50 por grupo**, ¿qué afirmación es
correcta?

-   La media de diferencias es **siempre** 0.

-   La media de diferencias es **siempre** distinta de 0.

-   Observar una diferencia de **±0.5** es **sorprendente** asumiendo
    H₀.

-   Observar una diferencia de **±0.1** es sorprendente asumiendo H₀.

**Q2.** ¿En qué se parecen y difieren los **modelos nulos** de las
Figuras 1.6 y 1.7?

-   (Pista: ambas centradas en 0; el **valor crítico** cambia con *n*.)

**Q3.** Explora la app (muestra, efecto, α). ¿Qué visualiza la zona
roja? ¿y la azul? (Tipo I y Tipo II / 1−potencia).

**Q4.** Con α = .01 (mantén n = 50, diferencia = 0.5), comparado con α =
.05:

-   Solo valores **menos extremos** son "sorprendentes", y el umbral
    pasa a **±0.53**.

-   ... a **±0.33**.

-   Solo valores **más extremos** son "sorprendentes", y el umbral pasa
    a **±0.53**.

-   ... a **±0.33**.

**Q5.** ¿Por qué **no** puedes concluir que H₀ es verdadera si
obtienes *p* \> α?

-   Debes reconocer la posibilidad de **Tipo II** (falso negativo).

**Q6.** ¿Por qué **no** puedes concluir que H₁ es verdadera si *p* \< α?

-   Debes reconocer la posibilidad de **Tipo I** (falso positivo) y
    que *p* **no** es prob. de hipótesis.

**Q7.** Con muestras **enormes** (p. ej., n = 50 000 por grupo), ¿qué
efectos serán "sorprendentes"? (pistas: umbral ≈ ±0.04).\
**Q8.** Simula en R: rnorm(50, 0, 1) da media 0.5; *t* de una muestra
contra 0 produce *p* = .03. ¿Probabilidad de "significativo por azar" si
H₀ es cierta?\
3% · 5% · 95% · 100%\
**Q9.** ¿Cuál es la afirmación correcta sobre **probabilidad de
replicación**?

-   1−*p* · 1−*p*×Pr(H₀) · **Potencia** (si hay efecto) o **α** (si no
    lo hay) · Potencia + α\
    **Q10.** ¿Un *p* = .65 implica que H₀ es verdadera?\
    **Q11.** ¿Forma correcta de presentar un *p* no significativo?

-   "La diferencia **no fue** estadísticamente distinta de 0."\
    **Q12.** ¿Un *p* \< .05 implica que H₀ es falsa?\
    **Q13.** ¿Efecto significativo = efecto **importante**? (pista:
    análisis coste--beneficio, tamaños de efecto).

**1.8.3 Preguntas abiertas**

1.  ¿Qué determina la **forma** de la distribución de *p*?

2.  ¿Cómo cambia cuando hay efecto real y aumenta *n*?

3.  ¿Qué es la **paradoja de Lindley**?

4.  ¿Cómo se distribuyen los *p* de **estadísticos continuos** cuando H₀
    es cierta?

5.  Definición **correcta** de *p*.

6.  ¿Por qué es **incorrecto** pensar que un *p* no significativo
    confirma H₀?

7.  ¿Por qué es incorrecto pensar que
    un *p* significativo **refuta** H₀?

8.  ¿Por qué es incorrecto pensar que *p* significativo
    = **importante**?

9.  ¿Por qué es incorrecto pensar que, si observas un resultado
    significativo, la prob. de **Tipo I** es 5%?

10. ¿Por qué es incorrecto pensar que **1−*p*** es la prob. de
    replicación?

11. Diferencias entre **Fisher** y **Neyman--Pearson** al
    interpretar *p*.

12. ¿Qué representa el **modelo nulo** (H₀) en NHST?

13. Si no puedes usar NHST para concluir **no hay efecto
    significativo/útil**, ¿qué enfoques usarías (p.
    ej., **equivalencia**, **región de irrelevancia**,
    etc.)? [Lakens]{.underline}

**Atribución y licencia**

Traducción no oficial de: **Lakens, D. (2022). *Improving Your
Statistical Inferences*** (Sección "1.8 Test Yourself"). Recurso
original con **licencia CC BY-NC-SA** (Atribución--No
Comercial--Compartir Igual). Fuente: libro online y página del capítulo.
